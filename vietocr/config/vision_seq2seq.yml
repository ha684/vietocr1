backbone: vision_transformer
cnn:
    image_size : 224
    patch_size : 16
    hidden: 512
    dim : 1024
    depth : 6
    heads : 16
    mlp_dim : 2048
    dropout : 0.1
    emb_dropout : 0.1
seq_modeling: seq2seq
transformer:
    encoder_hidden: 512
    decoder_hidden: 512
    img_channel: 512
    decoder_embedded: 512
    dropout: 0.1

optimizer:
    max_lr: 0.001
    pct_start: 0.1
